# create_train_data.py
# Builds and writes the Hangman training dataset (serial or parallel with sharding).
# Progress bars via tqdm. Safe to run standalone or import into other scripts.

import os, json, hashlib, logging, random, argparse, time, re, queue
from collections import Counter
from concurrent.futures import ProcessPoolExecutor
from multiprocessing import Manager
import numpy as np
from tqdm import tqdm

from hangman_commons import (
    ALPHABET, LETTER_TO_INDEX, TOKEN_VOCAB_SIZE, MAX_INCORRECT_TRIES,
    load_dictionary_words, group_words_by_length, encode_state_as_model_inputs,
    compute_letter_frequencies, compute_word_rarity
)

# ---------------- Logging ----------------
LOG_LEVEL = os.getenv("HANGMAN_LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO),
                    format="%(asctime)s %(levelname)s %(name)s: %(message)s")
log = logging.getLogger("hangman.dataset")

# ---------------- Defaults ----------------
DATASET_VERSION = 4
DEFAULT_MAX_LEN = 32
DEFAULT_MAX_WORDS = 100_000
DEFAULT_SAMPLES_PER_WORD = 2
DEFAULT_MAX_STEPS_PER_WORD = 8
DEFAULT_MAX_SAMPLES = 1_000_000
DEFAULT_WORKERS = max(1, (os.cpu_count() or 1) // 2)
RANDOM_SEED = 42

# Label processing knobs
DEFAULT_SMOOTH_EPS = 0.05     # probability smoothing for targets
DEFAULT_TOPM_LETTERS = 12     # restrict targets to top-M frequent letters
DEFAULT_RARITY_POWER = 0.5    # oversample rare-letter words


# ---------------- Helpers ----------------
def sha1_of_file(filepath, chunk_bytes=1 << 20) -> str:
    """Return SHA1 checksum of a file (used for dictionary integrity tracking)."""
    h = hashlib.sha1()
    with open(filepath, "rb") as f:
        while True:
            block = f.read(chunk_bytes)
            if not block:
                break
            h.update(block)
    return h.hexdigest()


def get_dataset_paths(data_dir: str):
    """Return dictionary of standard dataset file paths."""
    os.makedirs(data_dir, exist_ok=True)
    return {
        "npz": os.path.join(data_dir, "train_dataset.npz"),
        "meta": os.path.join(data_dir, "dataset_meta.json"),
        "clean_dict": os.path.join(data_dir, "dictionary_clean.txt"),
        "shard_prefix": os.path.join(data_dir, "train_dataset.shard"),
    }


def make_dataset_metadata(config: dict) -> dict:
    """Attach dataset version and seed to metadata dict."""
    meta = dict(config)
    meta["version"] = DATASET_VERSION
    meta["random_seed"] = RANDOM_SEED
    return meta


def apply_guesses_to_mask(word: str, guessed_letters: set[str]) -> str:
    """Return word masked with underscores for unguessed letters."""
    return "".join([c if c in guessed_letters else "_" for c in word])


# ---------------- Target Distribution ----------------
def compute_next_letter_distribution(mask: str, guessed_letters: set[str],
                                     words_by_length: dict[int, list[str]],
                                     smooth_eps: float, topm_letters: int) -> np.ndarray:
    """
    Compute probability distribution over next letters given a mask and guessed set.
    - Candidates: all words matching mask and excluding invalid guesses.
    - Counts letters in candidates, zeroing already guessed.
    - Optionally restricts to top-M unguessed letters.
    - Adds epsilon smoothing over unguessed letters.
    """
    # Candidate words
    regex = re.compile("^" + mask.replace("_", ".") + "$")
    present_letters = {c for c in mask if c != "_"}
    absent_letters = {g for g in guessed_letters if g not in present_letters}
    candidates = [
        w for w in words_by_length.get(len(mask), [])
        if regex.match(w) and not any(b in w for b in absent_letters)
    ]

    probs = np.zeros((26,), dtype=np.float32)

    if not candidates:
        # No candidates → fallback to uniform
        probs[:] = 1.0
    else:
        # Count letter frequencies across candidates
        counts = Counter("".join(candidates))
        for ch, cnt in counts.items():
            probs[LETTER_TO_INDEX[ch]] = float(cnt)

        # Restrict to top-M unguessed letters
        if topm_letters and topm_letters > 0:
            mask_unguessed = np.ones(26, dtype=bool)
            for ch in guessed_letters:
                if ch in LETTER_TO_INDEX:
                    mask_unguessed[LETTER_TO_INDEX[ch]] = False

            scores = probs.copy()
            scores[~mask_unguessed] = -1.0
            top_indices = np.argsort(scores)[-topm_letters:]
            keep_mask = np.zeros(26, dtype=bool)
            keep_mask[top_indices] = True
            probs[~keep_mask] = 0.0

        # Zero-out already guessed
        for ch in guessed_letters:
            if ch in LETTER_TO_INDEX:
                probs[LETTER_TO_INDEX[ch]] = 0.0

    # Add smoothing to unguessed
    if smooth_eps > 0:
        for i in range(26):
            if ALPHABET[i] not in guessed_letters:
                probs[i] += float(smooth_eps)

    # Normalize (fallback: uniform over unguessed)
    total = probs.sum()
    if total == 0.0:
        for i in range(26):
            probs[i] = 0.0 if ALPHABET[i] in guessed_letters else 1.0
        total = probs.sum()
    probs /= total
    return probs


# ---------------- Sample Generation ----------------
def generate_samples_for_word(word: str, samples_per_word: int, max_steps_per_word: int,
                              max_len_used: int, words_by_length: dict[int, list[str]],
                              rng: np.random.Generator, smooth_eps: float, topm_letters: int):
    """
    Generate multiple training samples for one word by simulating Hangman games.
    Returns lists of sequence one-hots, guessed vectors, meta features, and target distributions.
    """
    sequences, guessed_vectors, meta_features, target_distributions = [], [], [], []
    unique_letters = list(set(word))

    for _ in range(samples_per_word):
        guessed_letters, missed_count = set(), 0

        # Warm start: guess a few random letters from the word
        warm_start_count = rng.integers(0, 3)
        if unique_letters:
            warm_choices = rng.choice(unique_letters,
                                      size=min(warm_start_count, len(unique_letters)),
                                      replace=False)
            guessed_letters.update(warm_choices)

        steps = 0
        while steps < max_steps_per_word:
            steps += 1
            mask = apply_guesses_to_mask(word, guessed_letters)
            if "_" not in mask:
                break  # solved

            tries_left = MAX_INCORRECT_TRIES - missed_count
            target_probs = compute_next_letter_distribution(mask, guessed_letters,
                                                            words_by_length,
                                                            smooth_eps, topm_letters)

            seq_onehot, guess_vec, meta_vec = encode_state_as_model_inputs(
                mask, guessed_letters, tries_left, max_len_used
            )

            sequences.append(seq_onehot)
            guessed_vectors.append(guess_vec)
            meta_features.append(meta_vec)
            target_distributions.append(target_probs)

            # Choose argmax letter for simulation
            next_idx = int(np.argmax(target_probs))
            next_letter = ALPHABET[next_idx]
            guessed_letters.add(next_letter)

            if next_letter not in word:
                missed_count += 1
                if missed_count >= MAX_INCORRECT_TRIES:
                    break

    return sequences, guessed_vectors, meta_features, target_distributions


# ---------------- Shard Worker ----------------
def write_shard_from_words(shard_id: int, shard_words: list[str], samples_per_word: int,
                           max_steps_per_word: int, max_len_used: int,
                           words_by_length: dict[int, list[str]], per_worker_cap: int,
                           shard_prefix: str, base_seed: int,
                           smooth_eps: float, topm_letters: int,
                           progress_q=None, report_every=1000):
    """
    Worker process to generate training samples for a shard of words.
    Writes NPZ shard file with arrays for inputs and targets.
    """
    rng = np.random.default_rng(base_seed + shard_id * 1_000_003)
    random.seed(base_seed + shard_id * 1_000_003)

    sequences, guesses, metas, targets = [], [], [], []
    total_generated, pending_report = 0, 0

    for word in shard_words:
        seqs, gvecs, metas_list, targ_dists = generate_samples_for_word(
            word, samples_per_word, max_steps_per_word, max_len_used,
            words_by_length, rng, smooth_eps, topm_letters
        )
        n = len(targ_dists)
        if n:
            sequences.extend(seqs)
            guesses.extend(gvecs)
            metas.extend(metas_list)
            targets.extend(targ_dists)
            total_generated += n
            pending_report += n

        if progress_q and pending_report >= report_every:
            try:
                progress_q.put(pending_report)
            except Exception:
                pass
            pending_report = 0

        if total_generated >= per_worker_cap:
            break

    if progress_q and pending_report > 0:
        try:
            progress_q.put(pending_report)
        except Exception:
            pass

    shard_path = f"{shard_prefix}{shard_id:03d}.npz"
    if total_generated == 0:
        np.savez_compressed(
            shard_path,
            X_sequence=np.zeros((0, max_len_used, TOKEN_VOCAB_SIZE), dtype=np.float32),
            X_guessed_letters=np.zeros((0, 26), dtype=np.float32),
            X_meta_features=np.zeros((0, 2), dtype=np.float32),
            Y_next_letter_distribution=np.zeros((0, 26), dtype=np.float32),
        )
        return shard_path, 0

    np.savez_compressed(
        shard_path,
        X_sequence=np.stack(sequences),
        X_guessed_letters=np.stack(guesses),
        X_meta_features=np.stack(metas),
        Y_next_letter_distribution=np.stack(targets),
    )
    return shard_path, int(len(targets))


# ---------------- Merge Shards ----------------
def merge_shards(shard_files: list[str], out_path_npz: str, max_cap: int) -> int:
    """Merge shard NPZ files into one final dataset, respecting sample cap."""
    X_seq_list, X_guess_list, X_meta_list, Y_list = [], [], [], []
    total = 0

    for sf in sorted(shard_files):
        with np.load(sf) as z:
            xs, xg, xm, y = z["X_sequence"], z["X_guessed_letters"], z["X_meta_features"], z["Y_next_letter_distribution"]
        if xs.shape[0] == 0:
            continue
        remaining = max(0, max_cap - total)
        if remaining == 0:
            break
        take = min(remaining, xs.shape[0])
        X_seq_list.append(xs[:take])
        X_guess_list.append(xg[:take])
        X_meta_list.append(xm[:take])
        Y_list.append(y[:take])
        total += take

    if total == 0:
        X_sequence = np.zeros((0, 1, TOKEN_VOCAB_SIZE), dtype=np.float32)
        X_guessed_letters = np.zeros((0, 26), dtype=np.float32)
        X_meta_features = np.zeros((0, 2), dtype=np.float32)
        Y_next_letter_distribution = np.zeros((0, 26), dtype=np.float32)
    else:
        X_sequence = np.concatenate(X_seq_list, axis=0)
        X_guessed_letters = np.concatenate(X_guess_list, axis=0)
        X_meta_features = np.concatenate(X_meta_list, axis=0)
        Y_next_letter_distribution = np.concatenate(Y_list, axis=0)

    np.savez_compressed(
        out_path_npz,
        X_sequence=X_sequence,
        X_guessed_letters=X_guessed_letters,
        X_meta_features=X_meta_features,
        Y_next_letter_distribution=Y_next_letter_distribution,
    )
    return int(len(Y_next_letter_distribution))


# ---------------- Main Dataset Builder ----------------
def build_and_save_training_dataset(
    data_dir: str,
    dictionary_path: str,
    requested_max_len=DEFAULT_MAX_LEN,
    max_words=DEFAULT_MAX_WORDS,
    samples_per_word=DEFAULT_SAMPLES_PER_WORD,
    max_steps_per_word=DEFAULT_MAX_STEPS_PER_WORD,
    max_samples_cap=DEFAULT_MAX_SAMPLES,
    workers: int = 1,
    smooth_eps: float = DEFAULT_SMOOTH_EPS,
    topm_letters: int = DEFAULT_TOPM_LETTERS,
    rarity_power: float = DEFAULT_RARITY_POWER,
):
    """
    Build Hangman dataset: simulate games, produce training samples, and save NPZ + metadata.
    Can run in serial or parallel (sharded).
    """
    t_start = time.time()
    paths = get_dataset_paths(data_dir)
    dict_abs_path = os.path.abspath(dictionary_path)
    dict_sha1 = sha1_of_file(dict_abs_path)

    words = load_dictionary_words(dict_abs_path)
    log.info("Dictionary entries=%d", len(words))
    max_len_used = min(requested_max_len, max(len(w) for w in words))
    words_by_length = group_words_by_length(words)

    # Rarity-weighted shuffle of words
    rng = np.random.default_rng(RANDOM_SEED)
    freqs = compute_letter_frequencies(words)
    weights = np.array([compute_word_rarity(w, freqs) for w in words], dtype=np.float64)
    keys = rng.random(len(words)) ** (1.0 / (weights + 1e-12))  # Efraimidis–Spirakis
    ordered_words = [words[i] for i in np.argsort(keys)[::-1] if len(words[i]) <= max_len_used]
    if max_words is not None:
        ordered_words = ordered_words[:max_words]

    # --- Serial mode ---
    if workers <= 1:
        log.info("Running single-process dataset generation.")
        sequences, guesses, metas, targets = [], [], [], []
        total_samples = 0
        rng_local = np.random.default_rng(RANDOM_SEED)

        with tqdm(total=int(max_samples_cap), desc="Samples", unit="samp", dynamic_ncols=True) as pbar:
            for word in tqdm(ordered_words, desc="Words", unit="word", dynamic_ncols=True, leave=False):
                seqs, gvecs, mvecs, targs = generate_samples_for_word(
                    word, samples_per_word, max_steps_per_word,
                    max_len_used, words_by_length, rng_local,
                    smooth_eps, topm_letters
                )
                n = len(targs)
                if n:
                    sequences.extend(seqs); guesses.extend(gvecs)
                    metas.extend(mvecs); targets.extend(targs)
                    total_samples += n
                    pbar.update(min(n, max_samples_cap - total_samples))
                if total_samples >= max_samples_cap:
                    break

        X_sequence = np.stack(sequences) if total_samples > 0 else np.zeros((0, max_len_used, TOKEN_VOCAB_SIZE))
        X_guessed = np.stack(guesses) if total_samples > 0 else np.zeros((0, 26))
        X_meta = np.stack(metas) if total_samples > 0 else np.zeros((0, 2))
        Y_targets = np.stack(targets) if total_samples > 0 else np.zeros((0, 26))

        np.savez_compressed(paths["npz"],
                            X_sequence=X_sequence,
                            X_guessed_letters=X_guessed,
                            X_meta_features=X_meta,
                            Y_next_letter_distribution=Y_targets)

        with open(paths["clean_dict"], "w") as f:
            f.write("\n".join(words))

        meta = make_dataset_metadata({
            "max_len_used": int(max_len_used),
            "max_words": None if max_words is None else int(max_words),
            "samples_per_word": int(samples_per_word),
            "max_steps_per_word": int(max_steps_per_word),
            "max_samples_cap": int(max_samples_cap),
            "dict_path": dict_abs_path,
            "dict_sha1": dict_sha1,
            "workers": 1,
            "shards": [],
            "actual_samples": int(len(Y_targets)),
            "smooth_eps": float(smooth_eps),
            "topm_letters": int(topm_letters),
            "rarity_power": float(rarity_power),
        })
        with open(paths["meta"], "w") as f:
            json.dump(meta, f, indent=2)

        log.info("Dataset written: %s (samples=%d, max_len=%d) in %.1fs",
                 paths["npz"], len(Y_targets), max_len_used, time.time() - t_start)
        return (X_sequence, X_guessed, X_meta), Y_targets, max_len_used

    # --- Parallel mode ---
    workers = max(1, int(workers))
    word_splits = [[] for _ in range(workers)]
    for i, w in enumerate(ordered_words):
        word_splits[i % workers].append(w)

    per_worker_cap = int(np.ceil(max_samples_cap / workers))
    log.info("Parallel generation with %d workers; per-worker cap=%d", workers, per_worker_cap)

    manager = Manager()
    progress_q = manager.Queue()

    with tqdm(total=int(max_samples_cap), desc="Samples", unit="samp", dynamic_ncols=True) as pbar:
        with ProcessPoolExecutor(max_workers=workers) as ex:
            futures = [
                ex.submit(
                    write_shard_from_words,
                    sid, word_splits[sid], samples_per_word,
                    max_steps_per_word, max_len_used,
                    words_by_length, per_worker_cap, paths["shard_prefix"],
                    RANDOM_SEED, smooth_eps, topm_letters,
                    progress_q, 1000
                )
                for sid in range(workers)
            ]

            seen = 0
            while True:
                all_done = all(f.done() for f in futures)
                try:
                    while True:
                        inc = int(min(progress_q.get_nowait(), max(0, max_samples_cap - seen)))
                        if inc > 0:
                            pbar.update(inc)
                            seen += inc
                except queue.Empty:
                    pass
                if all_done:
                    break
                time.sleep(0.05)

            shard_files = []
            shard_counts = []
            for f in futures:
                shard_path, n = f.result()
                shard_files.append(shard_path)
                shard_counts.append(n)

    log.info("Shard sample counts: %s; total=%d", shard_counts, sum(shard_counts))
    final_samples = merge_shards(shard_files, paths["npz"], max_samples_cap)

    with open(paths["clean_dict"], "w") as f:
        f.write("\n".join(words))

    meta = make_dataset_metadata({
        "max_len_used": int(max_len_used),
        "max_words": None if max_words is None else int(max_words),
        "samples_per_word": int(samples_per_word),
        "max_steps_per_word": int(max_steps_per_word),
        "max_samples_cap": int(max_samples_cap),
        "dict_path": dict_abs_path,
        "dict_sha1": dict_sha1,
        "workers": int(workers),
        "shards": [os.path.basename(s) for s in sorted(shard_files)],
        "actual_samples": int(final_samples),
        "smooth_eps": float(smooth_eps),
        "topm_letters": int(topm_letters),
        "rarity_power": float(rarity_power),
    })
    with open(paths["meta"], "w") as f:
        json.dump(meta, f, indent=2)

    with np.load(paths["npz"]) as z:
        X_sequence, X_guessed, X_meta, Y_targets = (
            z["X_sequence"], z["X_guessed_letters"], z["X_meta_features"], z["Y_next_letter_distribution"]
        )

    log.info("Dataset written: %s (samples=%d, max_len=%d) in %.1fs",
             paths["npz"], len(Y_targets), max_len_used, time.time() - t_start)
    return (X_sequence, X_guessed, X_meta), Y_targets, max_len_used


# ---------------- CLI ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build Hangman training dataset.")
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument("--dictionary", type=str, default="words_250000_train.txt")
    parser.add_argument("--max_len", type=int, default=DEFAULT_MAX_LEN)
    parser.add_argument("--max_words", type=int, default=DEFAULT_MAX_WORDS)
    parser.add_argument("--samples_per_word", type=int, default=DEFAULT_SAMPLES_PER_WORD)
    parser.add_argument("--max_steps_per_word", type=int, default=DEFAULT_MAX_STEPS_PER_WORD)
    parser.add_argument("--max_samples_cap", type=int, default=DEFAULT_MAX_SAMPLES)
    parser.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    parser.add_argument("--smooth_eps", type=float, default=DEFAULT_SMOOTH_EPS)
    parser.add_argument("--topm_letters", type=int, default=DEFAULT_TOPM_LETTERS)
    parser.add_argument("--rarity_power", type=float, default=DEFAULT_RARITY_POWER)
    args = parser.parse_args()

    build_and_save_training_dataset(
        data_dir=args.data_dir,
        dictionary_path=args.dictionary,
        requested_max_len=args.max_len,
        max_words=args.max_words,
        samples_per_word=args.samples_per_word,
        max_steps_per_word=args.max_steps_per_word,
        max_samples_cap=args.max_samples_cap,
        workers=args.workers,
        smooth_eps=args.smooth_eps,
        topm_letters=args.topm_letters,
        rarity_power=args.rarity_power,
    )

# train_model.py
# CPU-only Hangman next-letter prediction model.
# Loads a cached dataset from --data_dir, trains the model, and saves artifacts.

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"   # Disable GPU usage
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

import json, time, argparse, logging, random, math, tempfile
from pathlib import Path
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, Concatenate, Flatten, Reshape
from tensorflow.keras.optimizers.schedules import CosineDecay

# AdamW optimizer import with fallbacks
try:
    from tensorflow.keras.optimizers import AdamW
except Exception:
    try:
        from tensorflow.keras.optimizers.legacy import AdamW
    except Exception:
        try:
            from tensorflow_addons.optimizers import AdamW
        except Exception:
            AdamW = None

from hangman_commons import TOKEN_VOCAB_SIZE
from hangman_layers import TransformerBlock, GLUBlock, PositionalEmbedding

# ---------------- Logging ----------------
LOG_LEVEL = os.getenv("HANGMAN_LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s %(levelname)s %(name)s: %(message)s"
)
logger = logging.getLogger("hangman.train")

# ---------------- File Paths ----------------
MODEL_SAVE_PATH = "hangman_model.keras"
MODEL_META_PATH = MODEL_SAVE_PATH + ".meta.json"

# ---------------- Default Hyperparameters ----------------
EMBEDDING_DIM = 128
ATTENTION_HEADS = 1
FEEDFORWARD_DIM = 256
NUM_TRANSFORMER_BLOCKS = 1
NUM_GLU_BLOCKS = 1
DEFAULT_BATCH_SIZE = 1024
DEFAULT_NUM_EPOCHS = 50
DEFAULT_STEPS_PER_EXEC = 64

# ---------------- Reproducibility ----------------
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.keras.utils.set_random_seed(RANDOM_SEED)
try:
    tf.config.experimental.enable_op_determinism(True)
except Exception:
    pass


# ---------------- Atomic Writers ----------------
def write_json_atomic(path: Path, obj: dict):
    """Write JSON to file atomically (safe against crashes)."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile(dir=str(path.parent), suffix=".json", delete=False, mode="w") as tmp:
        json.dump(obj, tmp)
        tmp.flush(); os.fsync(tmp.fileno())
        tmp_name = tmp.name
    os.replace(tmp_name, path)


def write_npy_atomic(path: Path, array: np.ndarray):
    """Write NumPy .npy array atomically."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile(dir=str(path.parent), suffix=".npy", delete=False) as tmp:
        np.save(tmp, array)
        tmp.flush(); os.fsync(tmp.fileno())
        tmp_name = tmp.name
    os.replace(tmp_name, path)


def write_npz_atomic(path: Path, **arrays):
    """Write compressed NumPy .npz archive atomically."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile(dir=str(path.parent), suffix=".npz", delete=False) as tmp:
        np.savez_compressed(tmp, **arrays)
        tmp.flush(); os.fsync(tmp.fileno())
        tmp_name = tmp.name
    os.replace(tmp_name, path)


# ---------------- Dataset Handling ----------------
def dataset_file_paths(data_dir: str) -> dict[str, str]:
    """Return expected dataset file paths."""
    return {
        "npz": os.path.join(data_dir, "train_dataset.npz"),
        "meta": os.path.join(data_dir, "dataset_meta.json")
    }


def load_cached_dataset(data_dir: str):
    """Load cached dataset (features + labels + metadata)."""
    paths = dataset_file_paths(data_dir)
    if not (os.path.isfile(paths["npz"]) and os.path.isfile(paths["meta"])):
        return None

    npz = np.load(paths["npz"])
    features_sequence = npz["X_sequence"]
    features_guessed = npz["X_guessed_letters"]
    features_meta = npz["X_meta_features"]
    labels_distribution = npz["Y_next_letter_distribution"]

    with open(paths["meta"], "r") as f:
        metadata = json.load(f)

    max_sequence_length = int(metadata["max_len_used"])
    logger.info(
        "Loaded dataset from %s (samples=%d, max_len=%d)",
        data_dir, len(labels_distribution), max_sequence_length
    )
    return (features_sequence, features_guessed, features_meta), labels_distribution, max_sequence_length


# ---------------- Model Architecture ----------------
def build_hangman_model(max_seq_len: int) -> tf.keras.Model:
    """Build transformer-based Hangman next-letter prediction model."""
    input_sequence = Input(shape=(max_seq_len, TOKEN_VOCAB_SIZE), name="masked_word_sequence")
    input_guessed = Input(shape=(26,), name="guessed_letters")
    input_meta = Input(shape=(2,), name="meta_features")

    # Project one-hot tokens into embedding space
    token_projection = Dense(EMBEDDING_DIM, activation=None, name="linear_projection")(input_sequence)
    token_with_position = PositionalEmbedding(max_len=max_seq_len, embed_dim=EMBEDDING_DIM, name="positional_encoding")(token_projection)

    x = token_with_position
    for i in range(NUM_TRANSFORMER_BLOCKS):
        x = TransformerBlock(
            embed_dim=EMBEDDING_DIM,
            num_heads=ATTENTION_HEADS,
            ff_dim=FEEDFORWARD_DIM,
            dropout=0.1,
            name=f"transformer_block_{i+1}"
        )(x)

    pooled_features = GlobalAveragePooling1D(name="average_pooling")(x)

    concatenated_features = Concatenate(name="concatenate_features")(
        [pooled_features, input_guessed, input_meta]
    )

    expanded_features = Reshape((1, EMBEDDING_DIM + 26 + 2), name="expand_time_dimension")(concatenated_features)

    h = Flatten(name="flatten_features")(expanded_features)
    h = Dense(256, activation="relu", name="dense_hidden")(h)
    h = Dropout(0.1, name="dropout_hidden")(h)
    output_logits = Dense(26, activation="softmax", name="predicted_letter_distribution")(h)

    return Model(inputs=[input_sequence, input_guessed, input_meta], outputs=output_logits)


# ---------------- TF Dataset Creation ----------------
def create_tf_datasets(train_inputs, y_train, val_inputs, y_val, batch_size: int):
    """Build tf.data.Dataset for training and validation."""
    AUTOTUNE = tf.data.AUTOTUNE
    n_train = len(y_train)

    train_dataset = tf.data.Dataset.from_tensor_slices(((train_inputs[0], train_inputs[1], train_inputs[2]), y_train))
    train_dataset = train_dataset.shuffle(min(n_train, 1_000_000), seed=RANDOM_SEED, reshuffle_each_iteration=True)
    train_dataset = train_dataset.batch(batch_size, drop_remainder=False).cache().prefetch(AUTOTUNE)

    val_dataset = tf.data.Dataset.from_tensor_slices(((val_inputs[0], val_inputs[1], val_inputs[2]), y_val))
    val_dataset = val_dataset.batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)

    return train_dataset, val_dataset


# ---------------- Training ----------------
def train_and_save_model(
    data_dir: str,
    batch_size: int,
    epochs: int,
    lr: float = 1e-3,
    weight_decay: float = 1e-3,
    cosine_alpha: float = 0.1,
    model_path: str = MODEL_SAVE_PATH,
    meta_path: str = MODEL_META_PATH,
    steps_per_execution: int = DEFAULT_STEPS_PER_EXEC,
    no_jit: bool = False,
    artifacts_dir: str = "artifacts",
):
    """Main training loop: load dataset, train model, save results."""
    start_time = time.time()

    cached = load_cached_dataset(data_dir)
    if cached is None:
        raise FileNotFoundError(
            f"Dataset not found in '{data_dir}'. Build it first:\n"
            f"python create_train_data.py --data_dir {data_dir} --dictionary words_250000_train.txt "
            f"--max_len 32 --max_words 100000 --samples_per_word 2 --max_steps_per_word 8 "
            f"--max_samples_cap 1000000 --workers 4"
        )

    (features_seq, features_guessed, features_meta), labels_distribution, max_len_used = cached
    labels_sparse = np.argmax(labels_distribution, axis=1).astype(np.int32)

    # Shuffle
    permutation = np.random.permutation(len(labels_sparse))
    features_seq, features_guessed, features_meta, labels_sparse = (
        features_seq[permutation],
        features_guessed[permutation],
        features_meta[permutation],
        labels_sparse[permutation]
    )

    # Train/val split
    n_train = int(0.8 * len(labels_sparse))
    train_inputs = [features_seq[:n_train], features_guessed[:n_train], features_meta[:n_train]]
    val_inputs   = [features_seq[n_train:], features_guessed[n_train:], features_meta[n_train:]]
    y_train, y_val = labels_sparse[:n_train], labels_sparse[n_train:]
    logger.info("Dataset sizes: train=%d val=%d", len(y_train), len(y_val))

    train_ds, val_ds = create_tf_datasets(train_inputs, y_train, val_inputs, y_val, batch_size)

    steps_per_epoch = max(1, math.ceil(n_train / batch_size))
    total_steps = steps_per_epoch * max(epochs, 1)
    lr_schedule = CosineDecay(initial_learning_rate=lr, decay_steps=total_steps, alpha=cosine_alpha)
    lr_values = np.array([float(lr_schedule(s).numpy()) for s in range(total_steps)], dtype=np.float32)

    optimizer = (
        tf.keras.optimizers.Adam(learning_rate=lr_schedule)
        if AdamW is None else AdamW(learning_rate=lr_schedule, weight_decay=weight_decay)
    )

    model = build_hangman_model(max_len_used)
    compile_config = dict(
        optimizer=optimizer,
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy")],
        run_eagerly=False,
        steps_per_execution=int(steps_per_execution),
    )

    if no_jit:
        model.compile(**compile_config)
        logger.info("XLA JIT: disabled")
    else:
        try:
            model.compile(**compile_config, jit_compile=True)
            logger.info("XLA JIT: enabled")
        except TypeError:
            model.compile(**compile_config)
            logger.warning("XLA JIT unsupported; continuing without it.")

    artifacts_path = Path(artifacts_dir)
    artifacts_path.mkdir(parents=True, exist_ok=True)

    # Train
    t1 = time.time()
    history_obj = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=1)
    t2 = time.time()

    # Save model + metadata
    model.save(model_path)
    write_json_atomic(Path(meta_path), {"max_len": int(max_len_used)})

    # Save training artifacts
    history_dict = {k: [float(x) for x in v] for k, v in history_obj.history.items()}
    write_json_atomic(artifacts_path / "history.json", history_dict)
    write_npy_atomic(artifacts_path / "lr_values.npy", lr_values)

    # Save validation predictions
    try:
        y_val_proba = model.predict(val_ds, verbose=0)
        y_val_pred = y_val_proba.argmax(axis=1).astype(np.int32)
        y_true = y_val.astype(np.int32)
        if y_val_proba.ndim != 2 or y_val_proba.shape[1] != 26:
            raise ValueError("Unexpected y_val_proba shape")
    except Exception as e:
        logger.warning("Validation prediction failed (%s). Writing empty val_preds.npz.", str(e))
        y_true = np.zeros((0,), dtype=np.int32)
        y_val_pred = np.zeros((0,), dtype=np.int32)
        y_val_proba = np.zeros((0, 26), dtype=np.float32)

    write_npz_atomic(artifacts_path / "val_preds.npz",
                     y_true=y_true, y_pred=y_val_pred, y_proba=y_val_proba)

    # Write sentinel DONE file
    write_json_atomic(artifacts_path / "DONE", {
        "finished_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "epochs": int(epochs),
        "steps_per_epoch": int(steps_per_epoch),
    })

    logger.info("Saved model=%s meta=%s", model_path, meta_path)
    logger.info("Artifacts saved to %s (history.json, lr_values.npy, val_preds.npz, DONE)", str(artifacts_path))
    logger.info("Steps/epoch=%d total_steps=%d lr=%.2e wd=%.1e spe=%d jit=%s",
                steps_per_epoch, total_steps, lr, weight_decay, steps_per_execution, str(not no_jit))
    logger.info("Timing: prep=%.1fs train=%.1fs total=%.1fs", t1 - start_time, t2 - t1, time.time() - start_time)


# ---------------- CLI ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Hangman next-letter model (CPU only). Dataset must exist.")
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument("--batch", type=int, default=DEFAULT_BATCH_SIZE)
    parser.add_argument("--epochs", type=int, default=DEFAULT_NUM_EPOCHS)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-3)
    parser.add_argument("--cosine_alpha", type=float, default=0.1)
    parser.add_argument("--steps_per_execution", type=int, default=DEFAULT_STEPS_PER_EXEC)
    parser.add_argument("--no_jit", action="store_true")
    parser.add_argument("--artifacts_dir", type=str, default="artifacts")
    args = parser.parse_args()

    train_and_save_model(
        data_dir=args.data_dir,
        batch_size=args.batch,
        epochs=args.epochs,
        lr=args.lr,
        weight_decay=args.weight_decay,
        cosine_alpha=args.cosine_alpha,
        steps_per_execution=args.steps_per_execution,
        no_jit=args.no_jit,
        artifacts_dir=args.artifacts_dir,
    )

# Shared Keras layers without Lambda ops. Safe to serialize/deserialize.
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention

@tf.keras.utils.register_keras_serializable(package="custom")
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim=512, num_heads=1, ff_dim=1024, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = int(embed_dim)
        self.num_heads = int(num_heads)
        self.ff_dim = int(ff_dim)
        self.dropout = float(dropout)
        self.proj = Dense(self.embed_dim)
        self.self_attn = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim // self.num_heads)
        self.drop_attn = Dropout(self.dropout)
        self.norm_attn = LayerNormalization(epsilon=1e-6)
        self.ffn_1 = Dense(self.ff_dim, activation=tf.keras.activations.gelu)
        self.ffn_2 = Dense(self.embed_dim, activation=tf.keras.activations.gelu)
        self.drop_ffn = Dropout(self.dropout)
        self.norm_ffn = LayerNormalization(epsilon=1e-6)

    def call(self, x, training=None):
        x_proj = x if x.shape[-1] == self.embed_dim else self.proj(x)
        attn_out = self.self_attn(x_proj, x_proj, training=training)
        x_res1 = self.norm_attn(x_proj + self.drop_attn(attn_out, training=training))
        ffn_out = self.ffn_2(self.ffn_1(x_res1))
        return self.norm_ffn(x_res1 + self.drop_ffn(ffn_out, training=training))

    def get_config(self):
        cfg = super().get_config()
        cfg.update(dict(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.ff_dim, dropout=self.dropout))
        return cfg

@tf.keras.utils.register_keras_serializable(package="custom")
class GLUBlock(tf.keras.layers.Layer):
    def __init__(self, units, dropout_rate=0.1, residual=True, **kwargs):
        super().__init__(**kwargs)
        self.units = int(units)
        self.dropout_rate = float(dropout_rate)
        self.residual = bool(residual)
        self.linear_signal = Dense(self.units, activation="linear")
        self.linear_gate = Dense(self.units, activation="sigmoid")
        self.drop = Dropout(self.dropout_rate)
        self.add = tf.keras.layers.Add()
        self.mul = tf.keras.layers.Multiply()

    def call(self, x, training=None):
        signal = self.linear_signal(x)
        gate = self.linear_gate(x)
        y = self.mul([signal, gate])
        y = self.drop(y, training=training)
        return self.add([x, y]) if self.residual else y

    def get_config(self):
        cfg = super().get_config()
        cfg.update(dict(units=self.units, dropout_rate=self.dropout_rate, residual=self.residual))
        return cfg

@tf.keras.utils.register_keras_serializable(package="custom")
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, max_len, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.max_len = int(max_len)
        self.embed_dim = int(embed_dim)
        self.pos_embed = tf.keras.layers.Embedding(self.max_len, self.embed_dim)

    def call(self, x):
        L = tf.shape(x)[1]
        pos = tf.range(L)
        pe = self.pos_embed(pos)[tf.newaxis, ...]
        return x + pe

    def get_config(self):
        cfg = super().get_config()
        cfg.update(dict(max_len=self.max_len, embed_dim=self.embed_dim))
        return cfg

@tf.keras.utils.register_keras_serializable(package="custom")
class AttentionPool1D(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.score = Dense(1, activation=None)

    def call(self, x):
        # x: (B, L, D)
        s = self.score(x)                   # (B, L, 1)
        a = tf.nn.softmax(s, axis=1)        # (B, L, 1)
        return tf.reduce_sum(a * x, axis=1) # (B, D)

    def get_config(self):
        return super().get_config()

# hangman_commons.py
# Core utilities for Hangman dataset creation and model preprocessing.

import re
import numpy as np

# ---------------- Alphabet and Tokenization ----------------
ALPHABET = "abcdefghijklmnopqrstuvwxyz"
LETTER_TO_INDEX = {c: i for i, c in enumerate(ALPHABET)}

UNDERSCORE_TOKEN_INDEX = 26  # index for "_"
PAD_TOKEN_INDEX = 27         # index for padding
TOKEN_VOCAB_SIZE = 28        # 26 letters + underscore + pad
MAX_INCORRECT_TRIES = 6      # max number of wrong guesses allowed


# ---------------- Dictionary Utilities ----------------
def load_dictionary_words(filepath: str) -> list[str]:
    """Read dictionary file and return a list of valid lowercase words (a–z only)."""
    with open(filepath, "r") as f:
        words = [w.strip().lower() for w in f if w.strip()]
    return [w for w in words if all("a" <= c <= "z" for c in w)]


def group_words_by_length(words: list[str]) -> dict[int, list[str]]:
    """Group words into buckets keyed by their length."""
    buckets: dict[int, list[str]] = {}
    for word in words:
        buckets.setdefault(len(word), []).append(word)
    return buckets


# ---------------- Mask Handling ----------------
def mask_to_regex(mask: str) -> str:
    """Convert a mask like 'a__le' into regex '^a..le$' for matching."""
    return "^" + mask.replace("_", ".") + "$"


def find_candidate_words(mask: str, guessed_letters: set[str], words_by_length: dict[int, list[str]]) -> list[str]:
    """
    Return all words of correct length consistent with a given mask and guessed letters.
    - mask: current masked word (e.g., 'a__le').
    - guessed_letters: set of already guessed characters.
    - words_by_length: dict of word lists grouped by length.
    """
    regex = re.compile(mask_to_regex(mask))
    present = {ch for ch in mask if ch != "_"}
    absent = {g for g in guessed_letters if g not in present}

    candidates = []
    for word in words_by_length.get(len(mask), []):
        if regex.match(word) and not any(b in word for b in absent):
            candidates.append(word)
    return candidates


# ---------------- Feature Encoding ----------------
def encode_state_as_model_inputs(mask: str, guessed_letters: set[str], tries_left: int, max_len: int):
    """
    Encode current game state into model input features:
    - seq: one-hot sequence of shape (max_len, TOKEN_VOCAB_SIZE)
    - guessed_vector: binary vector of guessed letters (26,)
    - meta: normalized features [word_length/max_len, tries_left/MAX_INCORRECT_TRIES]
    """
    word_length = len(mask)

    # Encode sequence with letters, underscores, and padding
    sequence_matrix = np.zeros((max_len, TOKEN_VOCAB_SIZE), dtype=np.float32)
    for i in range(max_len):
        if i >= word_length:
            sequence_matrix[i, PAD_TOKEN_INDEX] = 1.0
        else:
            ch = mask[i]
            if ch == "_":
                sequence_matrix[i, UNDERSCORE_TOKEN_INDEX] = 1.0
            else:
                sequence_matrix[i, LETTER_TO_INDEX[ch]] = 1.0

    # Encode guessed letters
    guessed_vector = np.zeros((26,), dtype=np.float32)
    for c in guessed_letters:
        if c in LETTER_TO_INDEX:
            guessed_vector[LETTER_TO_INDEX[c]] = 1.0

    # Encode meta features
    tries_normalized = max(0, min(MAX_INCORRECT_TRIES, int(tries_left))) / MAX_INCORRECT_TRIES
    meta_features = np.array([word_length / max_len, tries_normalized], dtype=np.float32)

    return sequence_matrix, guessed_vector, meta_features


# ---------------- Frequency & Rarity Helpers ----------------
def compute_letter_frequencies(words: list[str]) -> np.ndarray:
    """
    Compute normalized frequency distribution over 26 letters.
    Counts each letter once per word (avoiding length bias).
    """
    counts = np.zeros(26, dtype=np.float64)
    for word in words:
        for ch in set(word):
            if ch in LETTER_TO_INDEX:
                counts[LETTER_TO_INDEX[ch]] += 1.0
    counts = counts + 1e-8
    return counts / counts.sum()


def compute_word_rarity(word: str, letter_frequencies: np.ndarray, exponent: float = 0.5) -> float:
    """
    Compute rarity weight for a word: higher for words with rare letters.
    - word: input word
    - letter_frequencies: vector of size (26,) with letter frequencies
    - exponent: controls scaling (0.5 = sqrt weighting, 1.0 = linear inverse frequency)
    """
    inverse_freqs = 1.0 / letter_frequencies
    letter_indices = [LETTER_TO_INDEX[c] for c in set(word)]
    base_score = np.mean(inverse_freqs[letter_indices])
    return float(base_score ** exponent)

# inference_solver.py
# Inference-only Hangman solver with temperature scaling and candidate masking.

import os, json, logging
import numpy as np
import tensorflow as tf

from hangman_layers import TransformerBlock, GLUBlock, AttentionPool1D, PositionalEmbedding
from hangman_commons import (
    ALPHABET, LETTER_TO_INDEX, load_dictionary_words, group_words_by_length,
    encode_state_as_model_inputs, list_dictionary_candidates_for_mask
)

# ---------------- Logging ----------------
LOG_LEVEL = os.getenv("HANGMAN_LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s %(levelname)s %(name)s: %(message)s"
)
log = logging.getLogger("hangman.inference")

# ---------------- Defaults ----------------
DEFAULT_MODEL_PATH = "hangman_model.keras"
DEFAULT_META_PATH = DEFAULT_MODEL_PATH + ".meta.json"
DEFAULT_DICT_PATH = "words_250000_train.txt"


class HangmanSolver:
    """
    Neural Hangman solver with candidate filtering.

    Method:
        predict_next_letter(mask: str, guessed_letters: set[str],
                            tries_left: int, temperature: float = 1.0) -> str
    """

    def __init__(self,
                 model_path: str = DEFAULT_MODEL_PATH,
                 meta_path: str = DEFAULT_META_PATH,
                 dict_path: str = DEFAULT_DICT_PATH):
        # Load metadata
        with open(meta_path, "r") as f:
            meta = json.load(f)
        self.max_seq_len = int(meta["max_len"])

        # Load trained model with custom layers
        self.model = tf.keras.models.load_model(
            model_path,
            custom_objects={
                "TransformerBlock": TransformerBlock,
                "GLUBlock": GLUBlock,
                "AttentionPool1D": AttentionPool1D,
                "PositionalEmbedding": PositionalEmbedding,
            }
        )

        # Load and bucket dictionary words
        words = load_dictionary_words(dict_path)
        self.words_by_length = group_words_by_length(words)

        log.info("HangmanSolver loaded: model=%s max_len=%d dictionary_words=%d",
                 model_path, self.max_seq_len,
                 sum(len(v) for v in self.words_by_length.values()))

    # ---------------- Internal helpers ----------------
    @staticmethod
    def _apply_temperature_scaling(probs: np.ndarray, temperature: float) -> np.ndarray:
        """Apply temperature scaling to a probability distribution."""
        if temperature <= 0:
            return probs
        if abs(temperature - 1.0) < 1e-8:
            return probs
        adjusted = np.power(np.maximum(probs, 1e-12), 1.0 / float(temperature))
        return adjusted / adjusted.sum()

    # ---------------- Public API ----------------
    def predict_next_letter(self,
                            mask: str,
                            guessed_letters: set[str],
                            tries_left: int,
                            temperature: float = 1.0) -> str:
        """
        Predict the next letter for a given Hangman state.

        Args:
            mask: masked word (e.g., "s__ne")
            guessed_letters: set of letters already guessed
            tries_left: number of incorrect guesses remaining
            temperature: controls exploration (T>1 = smoother, T<1 = sharper)

        Returns:
            str: predicted letter
        """
        # Encode state into model inputs
        seq_onehot, guessed_vec, meta_vec = encode_state_as_model_inputs(
            mask, guessed_letters, tries_left, self.max_seq_len
        )
        probs = self.model.predict([seq_onehot[None], guessed_vec[None], meta_vec[None]], verbose=0)[0]

        # Suppress already-guessed letters
        for c in guessed_letters:
            if c in LETTER_TO_INDEX:
                probs[LETTER_TO_INDEX[c]] = 0.0

        # Restrict to letters consistent with candidate words
        candidates = list_dictionary_candidates_for_mask(mask, guessed_letters, self.words_by_length)
        if candidates:
            allowed = set("".join(candidates))
            for ch in ALPHABET:
                if ch not in allowed:
                    probs[LETTER_TO_INDEX[ch]] = 0.0

        # Apply temperature scaling
        probs = self._apply_temperature_scaling(probs, temperature)

        return ALPHABET[int(np.argmax(probs))]
